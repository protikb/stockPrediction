{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train a Bayesian LSTM on a sentiment classification task.\n",
    "# GPU command:\n",
    "#     THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python script.py\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/usr/local/cuda-7.0/bin\")\n",
    "sys.path.insert(0, \"../keras\") # point this to your local fork of https://github.com/yaringal/keras\n",
    "sys.path.insert(0, \"../Theano\")\n",
    "import theano\n",
    "# Create ram disk: mount -t tmpfs -o size=512m tmpfs /mnt/ramdisk\n",
    "# Use flag THEANO_FLAGS='base_compiledir=/mnt/ramdisk' python script.py\n",
    "print('Theano version: ' + theano.__version__ + ', base compile dir: '\n",
    "      + theano.config.base_compiledir)\n",
    "theano.config.mode = 'FAST_RUN'\n",
    "theano.config.optimizer = 'fast_run'\n",
    "theano.config.reoptimize_unpickled_function = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing import text\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding, DropoutEmbedding\n",
    "from keras.layers.recurrent import LSTM, GRU, DropoutLSTM, NaiveDropoutLSTM\n",
    "from keras.callbacks import ModelCheckpoint, ModelTest\n",
    "from keras.regularizers import l2\n",
    "seed = 0\n",
    "#adding some code here\n",
    "# importing csv data \n",
    "from numpy import genfromtxt\n",
    "\n",
    "inp_data = genfromtxt('/protik/pushGit/inp.csv', delimiter=',')\n",
    "out_data = genfromtxt('/protik/pushGit/out.csv', delimiter=',')\n",
    "#out_data = np.reshape(out_data,(198,1))\n",
    "out_data=out_data.tolist()\n",
    "#print(inp_data)\n",
    "#print(inp_data.shape)\n",
    "#print(out_data)\n",
    "#print(len(out_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "if len(sys.argv) == 1:\n",
    "  print(\"Expected args: p_W, p_U, p_dense, p_emb, weight_decay, batch_size, maxlen\")\n",
    "  print(\"Using default args:\")\n",
    "  sys.argv = [\"\", \"0.5\", \"0.5\", \"0.5\", \"0.5\", \"1e-6\", \"128\", \"100\"]#..............changed maxlen from 200 to 100\n",
    "args = [float(a) for a in sys.argv[1:]]\n",
    "print(args)\n",
    "p_W, p_U, p_dense, p_emb, weight_decay, batch_size, maxlen = args\n",
    "batch_size = int(batch_size)\n",
    "maxlen = int(maxlen)\n",
    "folder = \"/root/BayesianRNN/\"\n",
    "filename = (\"sa_DropoutLSTM_pW_%.2f_pU_%.2f_pDense_%.2f_pEmb_%.2f_reg_%f_batch_size_%d_cutoff_%d_epochs\"\n",
    "  % (p_W, p_U, p_dense, p_emb, weight_decay, batch_size, maxlen))\n",
    "print(filename)\n",
    "\n",
    "nb_words = 20000\n",
    "# maxlen = 20  # cut texts after this number of words (among top max_features most common words)\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "skip_top = 0\n",
    "\n",
    "test_split = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "\n",
    "print(\"Loading data...\")\n",
    "files = [\"Dennis+Schwartz\", \"James+Berardinelli\", \"Scott+Renshaw\", \"Steve+Rhodes\"]\n",
    "texts, ratings = [], []\n",
    "for file in files:\n",
    "    with open(\"scale_data/scaledata/\" + file + \"/subj.\" + file, \"r\") as f:\n",
    "        texts += list(f)\n",
    "    with open(\"scale_data/scaledata/\" + file + \"/rating.\" + file, \"r\") as f:\n",
    "        ratings += list(f)\n",
    "tokenizer = text.Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "Y = [float(rating) for rating in ratings]\n",
    "#print(Y)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(X)\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(Y)\n",
    "\n",
    "X = [[start_char] + [w + index_from for w in x] for x in X]\n",
    "\n",
    "new_X = []\n",
    "new_Y = []\n",
    "for x, y in zip(X, Y):\n",
    "#     if len(x) < maxlen:\n",
    "#         new_X.append(x)\n",
    "#         new_Y.append(y)\n",
    "    for i in xrange(0, len(x), maxlen):\n",
    "        new_X.append(x[i:i+maxlen])\n",
    "        new_Y.append(y)\n",
    "X = new_X\n",
    "Y = new_Y\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters: 0 (padding), 1 (start), 2 (OOV)\n",
    "\n",
    "X = [[oov_char if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n",
    "#print(Y)\n",
    "#print(len(Y))\n",
    "#X_train = X[:int(len(X)*(1-test_split))]\n",
    "#Y_train = Y[:int(len(X)*(1-test_split))]\n",
    "X_train = inp_data[:150]#......................added commentd out above two lines\n",
    "Y_train = out_data[:150]#..............addded\n",
    "mean_y_train = np.mean(Y_train)\n",
    "std_y_train = np.std(Y_train)\n",
    "Y_train = [(y - mean_y_train) / std_y_train for y in Y_train]\n",
    "\n",
    "#X_test = X[int(len(X)*(1-test_split)):]\n",
    "#Y_test = Y[int(len(X)*(1-test_split)):]\n",
    "X_test = inp_data[150:]#.....added, commented above two lines\n",
    "Y_test = out_data[150:]#......added\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "print(len(Y_train), 'train sequences Y')\n",
    "print (len(Y_test), 'test sequence Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(DropoutEmbedding(nb_words + index_from, 128, W_regularizer=l2(weight_decay), p=p_emb))\n",
    "model.add(DropoutLSTM(128, 128, truncate_gradient=maxlen, W_regularizer=l2(weight_decay),\n",
    "                      U_regularizer=l2(weight_decay),\n",
    "                      b_regularizer=l2(weight_decay),\n",
    "                      p_W=p_W, p_U=p_U))\n",
    "model.add(Dropout(p_dense))\n",
    "model.add(Dense(128, 1, W_regularizer=l2(weight_decay), b_regularizer=l2(weight_decay)))\n",
    "\n",
    "#optimiser = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=False)\n",
    "optimiser = 'adam'\n",
    "model.compile(loss='mean_squared_error', optimizer=optimiser)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# model.load_weights(\"/scratch/home/Projects/rnn_dropout/exps/DropoutLSTM_weights_00540.hdf5\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "print(\"Train...\")\n",
    "checkpointer = ModelCheckpoint(filepath=folder+filename+\".hdf5\",\n",
    "    verbose=1, append_epoch_name=True, save_every_X_epochs=50)\n",
    "modeltest_1 = ModelTest(X_train[:100], mean_y_train + std_y_train * np.atleast_2d(Y_train[:100]).T,\n",
    "                      test_every_X_epochs=1, verbose=0, loss='euclidean',\n",
    "                      mean_y_train=mean_y_train, std_y_train=std_y_train, tau=0.1)\n",
    "modeltest_2 = ModelTest(X_test, np.atleast_2d(Y_test).T, test_every_X_epochs=1, verbose=0, loss='euclidean',\n",
    "                      mean_y_train=mean_y_train, std_y_train=std_y_train, tau=0.1)\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=250,\n",
    "          callbacks=[checkpointer, modeltest_1, modeltest_2]) #\n",
    "# score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n",
    "# print('Test score:', score)\n",
    "# print('Test accuracy:', acc)\n",
    "\n",
    "# model.save_weights(folder+filename+\"_250.hdf5\", overwrite=True)\n",
    "\n",
    "standard_prob = model.predict(X_train, batch_size=500, verbose=1)\n",
    "print(np.mean(((mean_y_train + std_y_train * np.atleast_2d(Y_train).T)\n",
    "               - (mean_y_train + std_y_train * standard_prob))**2, 0)**0.5)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "standard_prob = model.predict(X_test, batch_size=500, verbose=1)\n",
    "#print(standard_prob)\n",
    "T = 50\n",
    "prob = np.array([model.predict_stochastic(X_test, batch_size=500, verbose=0)\n",
    "                 for _ in xrange(T)])\n",
    "prob_mean = np.mean(prob, 0)\n",
    "print(np.mean((np.atleast_2d(Y_test).T - (mean_y_train + std_y_train * standard_prob))**2, 0)**0.5)\n",
    "print(np.mean((np.atleast_2d(Y_test).T - (mean_y_train + std_y_train * prob_mean))**2, 0)**0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
